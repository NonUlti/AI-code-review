import { validateConfig, config } from "./config.js";
import { LLM_PROVIDERS, LLM_PROVIDER_NAMES } from "./constants/llm-providers.js";
import { AI_REVIEW_LABEL } from "./constants/defaults.js";
import * as gitlabClient from "./services/gitlab-client.js";
import * as ollamaClient from "./services/ollama-client.js";
import * as openaiClient from "./services/openai-client.js";
import * as codexClient from "./services/codex-client.js";
import * as mrProcessor from "./services/mr-processor.js";
import { createServer, startServer, setupGracefulShutdown } from "./server.js";
import type { LLMDependencies } from "./services/mr-processor.js";

const main = async (): Promise<void> => {
  const providerName = LLM_PROVIDER_NAMES[config.llm.provider];
  let modelName: string;
  if (config.llm.provider === LLM_PROVIDERS.OLLAMA) {
    modelName = config.ollama.model;
  } else if (config.llm.provider === LLM_PROVIDERS.OPENAI) {
    modelName = config.openai.model;
  } else {
    modelName = "CLI";
  }

  console.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
  console.log("â•‘       GitLab MR AI ë¦¬ë·° ìë™í™” ë„êµ¬                        â•‘");
  console.log(`â•‘       Powered by ${providerName} ${modelName.padEnd(32 - providerName.length)} â•‘`);
  console.log("â•‘       Mode: Webhook Server                                â•‘");
  console.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

  try {
    validateConfig();

    // GitLab ì˜ì¡´ì„± ìƒì„±
    const gitlabDeps = gitlabClient.createGitLabDependencies(config.gitlab.url, config.gitlab.token);

    // LLM ì˜ì¡´ì„± ìƒì„± (providerì— ë”°ë¼)
    let llmDeps: LLMDependencies;
    let llmModel: string;

    if (config.llm.provider === LLM_PROVIDERS.OLLAMA) {
      llmDeps = ollamaClient.createOllamaDependencies(config.ollama.url);
      llmModel = config.ollama.model;
    } else if (config.llm.provider === LLM_PROVIDERS.OPENAI) {
      llmDeps = openaiClient.createOpenAIDependencies(config.openai.apiKey, config.openai.baseURL);
      llmModel = config.openai.model;
    } else {
      llmDeps = codexClient.createCodexDependencies(config.codex.cliPath, config.codex.timeoutSeconds);
      llmModel = "codex-cli";
    }

    // LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    console.log(`\nğŸ” ${LLM_PROVIDER_NAMES[config.llm.provider]} ëª¨ë¸ í™•ì¸ ì¤‘...`);
    const isAvailable = await mrProcessor.checkLLMAvailability(llmDeps, config.llm.provider, llmModel);

    if (!isAvailable) {
      console.error(`âŒ ${LLM_PROVIDER_NAMES[config.llm.provider]} ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.`);
      process.exit(1);
    }
    console.log(`âœ“ ${LLM_PROVIDER_NAMES[config.llm.provider]} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥`);

    // ì²˜ë¦¬ ìƒíƒœ ìƒì„±
    const processingState = mrProcessor.createProcessingState();

    // ì„œë²„ ìƒì„±
    const server = createServer(
      {
        port: config.webhook.port,
        host: config.webhook.host,
        webhookSecret: config.webhook.secret,
      },
      {
        gitlabDeps,
        llmDeps,
        llmProvider: config.llm.provider,
        projectId: config.gitlab.projectId,
        aiReviewLabel: AI_REVIEW_LABEL,
        llmModel,
        processingState,
      }
    );

    // Graceful shutdown ì„¤ì •
    setupGracefulShutdown(server);

    // ì„œë²„ ì‹œì‘
    await startServer(server, {
      port: config.webhook.port,
      host: config.webhook.host,
      webhookSecret: config.webhook.secret,
    });

    console.log("\nâœ“ ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.");
    console.log("  Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n");
  } catch (error) {
    console.error("\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", error);

    if (error instanceof Error) {
      console.error(error.message);
    }

    process.exit(1);
  }
};

main();
