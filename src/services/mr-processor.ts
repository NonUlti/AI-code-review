import type { GitLabDependencies, OllamaDependencies, OpenAIDependencies, CodexDependencies } from "../types/dependencies.js";
import type { MergeRequest } from "../types/gitlab.js";
import type { LLMProvider } from "../types/llm.js";
import { LLM_PROVIDERS } from "../constants/llm-providers.js";
import * as gitlabClient from "./gitlab-client.js";
import * as ollamaClient from "./ollama-client.js";
import * as openaiClient from "./openai-client.js";
import * as codexClient from "./codex-client.js";
import { buildReviewPrompt } from "../utils/prompt-builder.js";

/**
 * LLM í´ë¼ì´ì–¸íŠ¸ íƒ€ì… (Ollama, OpenAI, ë˜ëŠ” Codex)
 */
export type LLMDependencies = OllamaDependencies | OpenAIDependencies | CodexDependencies;

/**
 * ì²˜ë¦¬ ì¤‘ì¸ MRì„ ì¶”ì í•˜ê¸° ìœ„í•œ ìƒíƒœ
 */
export interface ProcessingState {
  processing: Set<number>;
}

/**
 * ì²˜ë¦¬ ìƒíƒœë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 */
export const createProcessingState = (): ProcessingState => ({
  processing: new Set(),
});

/**
 * ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 */
const handleProcessingError = async (gitlabDeps: GitLabDependencies, projectId: string, mrIid: number, error: Error): Promise<void> => {
  try {
    const errorComment = `## âš ï¸ AI ë¦¬ë·° ì‹¤íŒ¨

AI ë¦¬ë·° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:

\`\`\`
${error.message}
\`\`\`

ë‚˜ì¤‘ì— ë‹¤ì‹œ ë¦¬ë·° ë°›ê¸°ë¥¼ ì›í•  ì‹œ ai-review ë¼ë²¨ì„ ì œê±°í•´ì£¼ì„¸ìš”.`;

    await gitlabClient.addComment(gitlabDeps, projectId, mrIid, errorComment);
  } catch (commentError) {
    console.error("ì˜¤ë¥˜ ì½”ë©˜íŠ¸ ì¶”ê°€ ì‹¤íŒ¨:", commentError);
  }
};

/**
 * ë‹¨ì¼ MRì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 */
export const processSingleMR = async (
  gitlabDeps: GitLabDependencies,
  llmDeps: LLMDependencies,
  llmProvider: LLMProvider,
  projectId: string,
  aiReviewLabel: string,
  llmModel: string,
  mr: MergeRequest,
  state: ProcessingState
): Promise<void> => {
  state.processing.add(mr.iid);

  try {
    console.log(`\nğŸ“ MR !${mr.iid} ì²˜ë¦¬ ì‹œì‘: ${mr.title}`);

    const changes = await gitlabClient.getMergeRequestChanges(gitlabDeps, projectId, mr.iid);

    if (changes.length === 0) {
      console.log(`â­ï¸  MR !${mr.iid}ì— ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.`);
      return;
    }

    console.log(`âœ“ ${changes.length}ê°œì˜ íŒŒì¼ ë³€ê²½ ë°œê²¬`);

    // diff í¬ê¸° ë¡œê¹…
    const totalDiffSize = changes.reduce((sum, c) => sum + c.diff.length, 0);
    const sizeInKB = (totalDiffSize / 1024).toFixed(1);
    console.log(`ğŸ“Š ì „ì²´ diff í¬ê¸°: ${sizeInKB}KB`);

    const prompt = buildReviewPrompt(mr, changes);

    console.log(`ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ AI ë¦¬ë·° ìš”ì²­ ì¤‘...`);
    
    let review: string;
    if (llmProvider === LLM_PROVIDERS.OLLAMA) {
      review = await ollamaClient.queryOllamaModelStream(
        llmDeps as OllamaDependencies,
        llmModel,
        prompt,
        () => {} // ì²­í¬ëŠ” ë¬´ì‹œí•˜ê³  ì „ì²´ ì‘ë‹µë§Œ ìˆ˜ì§‘
      );
    } else if (llmProvider === LLM_PROVIDERS.OPENAI) {
      review = await openaiClient.queryOpenAIModelStream(
        llmDeps as OpenAIDependencies,
        llmModel,
        prompt,
        () => {} // ì²­í¬ëŠ” ë¬´ì‹œí•˜ê³  ì „ì²´ ì‘ë‹µë§Œ ìˆ˜ì§‘
      );
    } else {
      review = await codexClient.queryCodexModelStream(
        llmDeps as CodexDependencies,
        prompt,
        () => {} // ì²­í¬ëŠ” ë¬´ì‹œí•˜ê³  ì „ì²´ ì‘ë‹µë§Œ ìˆ˜ì§‘
      );
    }

    await gitlabClient.addComment(gitlabDeps, projectId, mr.iid, review);

    console.log(`âœ… MR !${mr.iid} ì²˜ë¦¬ ì™„ë£Œ\n`);
  } catch (error) {
    console.error(`âŒ MR !${mr.iid} ì²˜ë¦¬ ì‹¤íŒ¨:`, error);

    if (error instanceof Error) {
      await handleProcessingError(gitlabDeps, projectId, mr.iid, error);
    }
  } finally {
    try {
      await gitlabClient.addAiReviewLabel(gitlabDeps, projectId, mr.iid, aiReviewLabel);
    } catch (labelError) {
      console.error(`ë¼ë²¨ ì¶”ê°€ ì‹¤íŒ¨:`, labelError);
    }
    state.processing.delete(mr.iid);
  }
};

/**
 * ëŒ€ìƒ MRë“¤ì„ ì°¾ì•„ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 */
export const processMergeRequests = async (
  gitlabDeps: GitLabDependencies,
  llmDeps: LLMDependencies,
  llmProvider: LLMProvider,
  projectId: string,
  aiReviewLabel: string,
  excludeTargetBranches: string[],
  llmModel: string,
  state: ProcessingState
): Promise<void> => {
  try {
    console.log("\nğŸ” AI ë¦¬ë·° ëŒ€ìƒ MR ê²€ìƒ‰ ì¤‘...");

    const targetMRs = await gitlabClient.getTargetMergeRequests(
      gitlabDeps,
      projectId,
      aiReviewLabel,
      excludeTargetBranches
    );

    if (targetMRs.length === 0) {
      console.log("â„¹ï¸  ì²˜ë¦¬í•  MRì´ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    console.log(`âœ“ ${targetMRs.length}ê°œì˜ MR ë°œê²¬`);

    for (const mr of targetMRs) {
      if (state.processing.has(mr.iid)) {
        console.log(`â­ï¸  MR !${mr.iid}ëŠ” ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.`);
        continue;
      }

      await processSingleMR(gitlabDeps, llmDeps, llmProvider, projectId, aiReviewLabel, llmModel, mr, state);
    }
  } catch (error) {
    console.error("MR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", error);
  }
};

/**
 * LLM ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
 */
export const checkLLMAvailability = async (
  llmDeps: LLMDependencies,
  llmProvider: LLMProvider,
  model: string
): Promise<boolean> => {
  if (llmProvider === LLM_PROVIDERS.OLLAMA) {
    return ollamaClient.checkModelAvailability(llmDeps as OllamaDependencies, model);
  } else if (llmProvider === LLM_PROVIDERS.OPENAI) {
    return openaiClient.checkModelAvailability(llmDeps as OpenAIDependencies, model);
  } else {
    return codexClient.checkModelAvailability(llmDeps as CodexDependencies);
  }
};
