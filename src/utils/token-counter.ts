import { encoding_for_model, get_encoding, TiktokenModel } from "tiktoken";

/**
 * í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
 */
export interface TokenUsage {
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
  // ì…ë ¥ í¬ê¸° ì •ë³´
  promptCharacters: number;
  promptLines: number;
  // ì¶œë ¥ í¬ê¸° ì •ë³´
  completionCharacters: number;
  completionLines: number;
  // í† í° íš¨ìœ¨ (ë¹„ìœ¨)
  tokensPerCharacter: number;  // ë¬¸ìë‹¹ í† í° ìˆ˜ (ì˜ˆ: 0.25)
  tokensPerLine: number;       // ë¼ì¸ë‹¹ í† í° ìˆ˜ (ì˜ˆ: 8.5)
  charactersPerToken: number;  // í† í°ë‹¹ ë¬¸ì ìˆ˜ (ì˜ˆ: 4.0)
}

/**
 * ëª¨ë¸ë³„ ì¸ì½”ë”© ë§¤í•‘
 * - cl100k_base: GPT-4, GPT-3.5-turbo, text-embedding-ada-002
 * - o200k_base: GPT-4o, o1, GPT-5 ê³„ì—´ (ìµœì‹  ëª¨ë¸)
 */
const MODEL_ENCODING_MAP: Record<string, string> = {
  // GPT-4 ê³„ì—´
  "gpt-4": "cl100k_base",
  "gpt-4-turbo": "cl100k_base",
  "gpt-4-32k": "cl100k_base",
  // GPT-4o ë° ìµœì‹  ëª¨ë¸ (o200k_base)
  "gpt-4o": "o200k_base",
  "gpt-4o-mini": "o200k_base",
  "o1": "o200k_base",
  "o1-mini": "o200k_base",
  "o1-preview": "o200k_base",
  // GPT-5 ê³„ì—´ (o200k_base ì˜ˆìƒ)
  "gpt-5": "o200k_base",
  "gpt-5.2": "o200k_base",
  // GPT-3.5 ê³„ì—´
  "gpt-3.5-turbo": "cl100k_base",
};

// ìµœì‹  ëª¨ë¸ìš© ê¸°ë³¸ ì¸ì½”ë”© (GPT-4o, GPT-5 ê³„ì—´)
const DEFAULT_ENCODING = "o200k_base";

/**
 * í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
 * @param text í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
 * @param model ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-4o)
 * @returns í† í° ìˆ˜
 */
export const countTokens = (text: string, model: string = "gpt-4o"): number => {
  try {
    // ë¨¼ì € tiktokenì—ì„œ ì§ì ‘ ëª¨ë¸ ì§€ì› ì—¬ë¶€ í™•ì¸
    const enc = encoding_for_model(model as TiktokenModel);
    const tokens = enc.encode(text);
    const tokenCount = tokens.length;
    enc.free();
    return tokenCount;
  } catch {
    // ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° ë§¤í•‘ ë˜ëŠ” ê¸°ë³¸ ì¸ì½”ë”© ì‚¬ìš©
    const encodingName = MODEL_ENCODING_MAP[model.toLowerCase()] || DEFAULT_ENCODING;
    const enc = get_encoding(encodingName as "cl100k_base" | "o200k_base");
    const tokens = enc.encode(text);
    const tokenCount = tokens.length;
    enc.free();
    return tokenCount;
  }
};

/**
 * í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
 * @param prompt í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
 * @param completion ì‘ë‹µ í…ìŠ¤íŠ¸
 * @param model ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: gpt-4o)
 * @returns í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
 */
export const calculateTokenUsage = (
  prompt: string,
  completion: string,
  model: string = "gpt-4o"
): TokenUsage => {
  const promptTokens = countTokens(prompt, model);
  const completionTokens = countTokens(completion, model);
  const totalTokens = promptTokens + completionTokens;
  
  // í¬ê¸° ì •ë³´ ê³„ì‚°
  const promptCharacters = prompt.length;
  const promptLines = prompt.split('\n').length;
  const completionCharacters = completion.length;
  const completionLines = completion.split('\n').length;
  
  // ì „ì²´ ë¬¸ì ìˆ˜ì™€ ë¼ì¸ ìˆ˜
  const totalCharacters = promptCharacters + completionCharacters;
  const totalLines = promptLines + completionLines;
  
  // í† í° íš¨ìœ¨ ê³„ì‚°
  const tokensPerCharacter = totalCharacters > 0 ? totalTokens / totalCharacters : 0;
  const tokensPerLine = totalLines > 0 ? totalTokens / totalLines : 0;
  const charactersPerToken = totalTokens > 0 ? totalCharacters / totalTokens : 0;
  
  return {
    promptTokens,
    completionTokens,
    totalTokens,
    promptCharacters,
    promptLines,
    completionCharacters,
    completionLines,
    tokensPerCharacter,
    tokensPerLine,
    charactersPerToken,
  };
};

/**
 * í† í° ì‚¬ìš©ëŸ‰ì„ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.
 */
export const logTokenUsage = (usage: TokenUsage): void => {
  console.log("\nğŸ“Š í† í° ì‚¬ìš©ëŸ‰:");
  console.log(`  ğŸ“¥ í”„ë¡¬í”„íŠ¸: ${usage.promptTokens.toLocaleString()} í† í° (${usage.promptCharacters.toLocaleString()}ì, ${usage.promptLines.toLocaleString()}ì¤„)`);
  console.log(`  ğŸ“¤ ì‘ë‹µ: ${usage.completionTokens.toLocaleString()} í† í° (${usage.completionCharacters.toLocaleString()}ì, ${usage.completionLines.toLocaleString()}ì¤„)`);
  console.log(`  ğŸ“ˆ ì´í•©: ${usage.totalTokens.toLocaleString()} í† í°`);
  console.log("\nğŸ“ í† í° íš¨ìœ¨:");
  console.log(`  ğŸ”¤ ë¬¸ìë‹¹: ${usage.tokensPerCharacter.toFixed(3)} í† í° (â‰ˆ ${usage.charactersPerToken.toFixed(1)}ì/í† í°)`);
  console.log(`  ğŸ“ ë¼ì¸ë‹¹: ${usage.tokensPerLine.toFixed(1)} í† í°`);
};

/**
 * ì˜ˆìƒ ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤ (ì°¸ê³ ìš© - API ê°€ê²© ê¸°ì¤€)
 * ì£¼ì˜: ChatGPT Plus êµ¬ë…ì€ í† í° ê¸°ë°˜ ê³¼ê¸ˆì´ ì•„ë‹ˆë¯€ë¡œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤.
 */
export const estimateCost = (usage: TokenUsage, model: string = "gpt-4"): string => {
  // GPT-4 API ê¸°ì¤€ ê°€ê²© (2024ë…„ ê¸°ì¤€, ì°¸ê³ ìš©)
  const prices: Record<string, { input: number; output: number }> = {
    "gpt-4": { input: 0.03, output: 0.06 }, // per 1K tokens
    "gpt-4-turbo": { input: 0.01, output: 0.03 },
    "gpt-4o": { input: 0.005, output: 0.015 },
    "gpt-3.5-turbo": { input: 0.0005, output: 0.0015 },
  };

  const price = prices[model] || prices["gpt-4"];
  const inputCost = (usage.promptTokens / 1000) * price.input;
  const outputCost = (usage.completionTokens / 1000) * price.output;
  const totalCost = inputCost + outputCost;

  return `$${totalCost.toFixed(4)} (API ì‚¬ìš© ì‹œ ì˜ˆìƒ ë¹„ìš©)`;
};
